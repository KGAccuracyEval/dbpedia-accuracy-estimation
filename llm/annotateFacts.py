import time
import json
import torch
import argparse

from tqdm import tqdm
from prompts import *
from urllib.parse import urlparse
from huggingface_hub import login
from transformers import pipeline, set_seed, AutoTokenizer


parser = argparse.ArgumentParser()

############################
###### LLM parameters ######
############################

parser.add_argument('--llm', default='llama3-8b', type=str, help='LLM of choice.')
parser.add_argument('--temp', default=0, type=float, help='The LLM level of randomness in responses.')
parser.add_argument('--sample', default=False, type=bool, help='Enable decoding strategies such as multinomial sampling or beam search.')
parser.add_argument('--maxTokens', default=2, type=int, help='The maximum number of tokens generated by LLM.')

##############################
###### Other parameters ######
##############################

parser.add_argument('--token', default='', type=str, help='HuggingFace login token.')
parser.add_argument('--device', default='mps', type=str, help='Computing device considered for LLM usage -- mps = metal performance shaders (apple).')
parser.add_argument('--maxRetries', default=3, type=int, help='Number of retries the LLM is allowed to take')
parser.add_argument('--seed', default=42, type=int, help='Random seed.')

args = parser.parse_args()


# set dict that maps LLM names to corresponding HuggingFace IDs
model2id = {
    'llama3-8b': 'meta-llama/Meta-Llama-3-8B-Instruct',
    'gemma-7b': 'google/gemma-1.1-7b-it',
    'mistral-7b': 'mistralai/Mistral-7B-Instruct-v0.2',
}


def isURL(item):
    try:  # if urlparse manages to parse item, then item is URL -- return TRUE
        result = urlparse(item)
        return all([result.scheme, result.netloc])
    except:  # otherwise, return FALSE
        return False


def generateResponse(pipeLLM, chat):
    """
    Use LLM (pipeLLM) to generate response given input chat

    :param pipeLLM: LLM pipeline
    :param chat: input messages
    :return: LLM response
    """

    # apply chat template
    prompt = pipeLLM.tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)

    # obtain outputs
    outputs = pipeLLM(
        prompt,
        max_new_tokens=args.maxTokens,
        do_sample=args.sample,
        temperature=args.temp
    )

    # get response from outputs
    response = outputs[0]["generated_text"][len(prompt):]
    return response


def checkResponse4Correctness(response):
    """
    Check whether the LLM response contains one of the provided labels

    :param response: the LLM generated response
    :return: boolean
    """

    # lowercase response
    response = response.lower()
    # set labels
    labels = ['incorrect', 'correct', 'idk']
    for label in labels:
        if label in response:  # label found in response -- return True
            return True
    # label not found in response -- return False
    return False


def convertResponse2Correctness(response):
    """
    Convert (correct) response to agreement label

    :param response: the LLM generated response
    :return: label
    """

    # lowercase response
    response = response.lower()

    if 'incorrect' in response:  # return incorrect
        return 'incorrect'
    elif 'correct' in response:  # return correct
        return 'correct'
    elif 'idk' in response:  # return idk
        return 'idk'
    else:  # return na
        return 'na'


def main(model, gen_limit):
    # set args.llm = model
    args.llm = model
    args.maxTokens = gen_limit

    # set seed
    set_seed(args.seed)

    # set LLM as HF pipeline
    pipeLLM = pipeline(
        "text-generation",
        model=model2id[args.llm],
        tokenizer=AutoTokenizer.from_pretrained(model2id[args.llm]),
        model_kwargs={"torch_dtype": torch.bfloat16},
        device_map=args.device,
        trust_remote_code=True
    )

    # set prompts
    initial_prompt = CORRECTNESS_PROMPT
    retry_prompt = RETRY_PROMPT
    system_prompt = SYSTEM_PROMPT

    # open dataset
    with open('../data/dataset/llm/kg.json', 'r') as f:
        kg = json.load(f)

    # set LLM generated ground-truth var
    gtLLM = {}

    print(f'Annotating KG facts w/ {args.llm} and setting max tokens={args.maxTokens} ...')
    for factID, fact in tqdm(kg.items()):
        # clean up fact items -- keep only resource names
        s = fact[0].split('/')[-1]
        p = fact[1].split('/')[-1].split("#")[-1]
        if isURL(fact[2]):
            if fact[2].endswith('/'):
                o = fact[2]
            else:
                o = fact[2].split('/')[-1]
        else:
            o = fact[2]

        # concatenate fact into one string
        cFact = ' '.join((s, p, o))

        # start counting time
        start = time.time()

        # set messages
        messages = []
        if args.llm == 'llama3-8b':  # system role supported by llama3 model only
            messages += [{"role": "system", "content": system_prompt}]
        messages += [{"role": "user", "content": initial_prompt.format(fact=cFact)}]

        # generate response
        response = generateResponse(pipeLLM, messages)

        # set number of retries
        retries = 0
        while not checkResponse4Correctness(response):  # LLM response diverges from label -- ask again
            if retries == args.maxRetries:  # stop once number of retries reaches max number allowed
                response = 'na'
                break

            # set messages
            messages += [
                {"role": "assistant", "content": response},
                {"role": "user", "content": retry_prompt.format(chances=args.maxRetries-retries, fact=cFact)}
            ]

            # generate response
            response = generateResponse(pipeLLM, messages)

            # increment number of retries
            retries += 1

        # stop counting time
        end = time.time()

        # get label from response
        label = convertResponse2Correctness(response)
        # get duration
        duration = end-start

        # assign label to ground-truth
        gtLLM[factID] = {'label': label, 'time': duration, 'retries': retries}
    print(f'KG facts annotated w/ {args.llm}!')

    print('Store LLM annotations')
    with open('../data/annotations/llms/'+args.llm+'.json', 'w') as out:
        json.dump(gtLLM, out)
    print('LLM annotations stored!')


if __name__ == "__main__":
    # set HF login
    login(args.token)

    # set LLMs and the corresponding max tokens used to limit text generation
    models = ['llama3-8b', 'mistral-7b', 'gemma-7b']
    gen_limits = [2, 2, 10]

    for llm, limit in zip(models, gen_limits):  # iterate over LLMs and annotate facts
        main(llm, limit)
